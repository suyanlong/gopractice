package main

import (
	"encoding/binary"
	"fmt"
)

// 三个比较好的网址.
// http://www.cnblogs.com/xkfz007/articles/2566434.html
// https://www.zhihu.com/question/52346583
// https://blog.csdn.net/longintchar/article/details/51079340

// Unicode是一种编码集, 类是化学元素周期表一样.
// Unicode定义的每一个字符,都对唯一对应一个数字, 范围为:0-0x10FFFF,最多可以容纳1114112个字符
// 而,UTF-8、UTF-16、UTF-32是一种编码方式, 即翻译Unicode字符集转换格式,
// 即怎样将Unicode定义的数字转换成程序数据.
// 然而,会有人问,为什么不直接把unicode字符集对应的数字翻译成16进制呢? 而需要utf-8这种编码方式.???
// 答: 因为unicode没有定义. 但是如果依旧把字符代码当作字符编码来用，传输效率会非常低， Unicode本身的字符集是32位的，那么每个字符都用4个字节传输

// Unicode与UTF
// 从直觉上来看，我们觉得字符代码和字符编码应当是一样的，在大多数情况下也的确如此，
// 例如ASCII，因此其实长期以来我们混淆字符代码和字符编码概念时，也没觉得有什么不对。
// 但是在Unicode里会有缺陷，Unicode字符集解决了通用的问题，
// 但是如果依旧把字符代码当作字符编码来用，传输效率会非常低，
// Unicode本身的字符集是32位的，那么每个字符都用4个字节传输，对欧美而言，一下就要用原来ASCII码时代四倍的资源传输或者保存一样的文档。
// 对中国人而言也是一样的，原先的GB虽然可能在打一些不常用字上有缺陷，但是也只要两个字节保存或传输一个字符，现在一下大一倍，是不可接受的。
// 因此Unicode在推出之后推广相当困难。直到互联网时代的到来，跨语言信息交互空前频繁，统一编码大势所趋，才被真正铺开。
// 耗费资源的问题最终也被解决，解决的方案就是UTF（事实上UTF中的UTF-8标准在93年就被提出，但是似乎很多程序员都表示这是在互联网时代才兴起的编码方式）

// 不同语言对待字符,有不同的编码方式.
// python, golang的编码方式是utf-8,
// C/C++编码方式是多字节字符集.

// 而英文字符集,都是属于ASII编码,这个字符集都是一模一样,所以不会存在乱码现象.unicode的utf-8编码兼容ASCII编码方式.
// 因此在不同编码的程序(一个是ASCII,一个是utf-8)交互时,只要使用的都是英文单词等,就不会出现乱码,但是如果是中文就会乱码!!!
// 大端\小端字节序,是CPU寻址位大小,转化的的事情.与编码方式并无实际关系.

// 字节序与字符编码
// 对于任何字符编码，编码单元之间的顺序是由编码方式指定的，与endian无关。
// 例如GB2312的编码单元是字节，用两个字节表示一个字符。这两个字节的顺序是固定的，不受CPU字节序的影响。

// 什么是编码单元?
// 编码单元: 对代码点进行编码后的二进制的比特序列，是由一个或者多个编码单元按照一定的序列组成的。
// 一个编码单元由若干个比特构成，例如7比特、8比特等(最常用的是8/16/32比特，便于存储和传输)。
// 比如说，UTF-8编码，采用1~4个8比特的编码单元；
// UTF-16编码，采用1~2个16比特的编码单元；
// UTF-32编码，采用1个32比特的编码单元。
// 前两个属于变长编码，后一个属于等长编码。

// 因此utf-8编码的字节,不需要网络字节序的转换,
// 所以,传递utf-8编码的字节序列的字符串,不用担心大小端问题,所以类似utf-8编码的json\xml格式的字符串字节流不需要要注意大小端.

// 不同体系结构的CPU，数据在内存中存放的排列顺序是不一样的。
// 小端按小端方式存储与读取,大端按大端方式读取与存储.

func main() {
	fmt.Println("decode")
	a := []byte("A")
	fmt.Printf("%v\n", a)
	str := []byte("汉")
	fmt.Printf("%X\n", str)
	fmt.Printf("%U\n", str)

	str = []byte("中")
	fmt.Printf("%X\n", str)
	fmt.Printf("%U\n", str)

	str = []byte("文")
	fmt.Printf("%X\n", str)
	fmt.Printf("%U\n", str) // [U+00E6 U+0096 U+0087]

	// 我们把str[0]转换成数字,在0-255的范围.
	fmt.Println(str[0]) // 230, 在0-255的范围.

	//
	fmt.Println(binary.BigEndian.Uint16(str))

	// 大端与小端是存储方式不一样,是由CPU决定的.
	// 内存地址读取是安装从小到大的递增方式读取, 指针递增,进行偏移就是这样的.
	var btyeInt = make([]byte, 8)
	binary.BigEndian.PutUint64(btyeInt, 16)//[0 0 0 0 0 0 0 16]
	fmt.Println(btyeInt)

	//大端机器直接发送一个0x0000 0000 0000 00FF给小端, 小端不经过转换,以为也是小端的数据,
	//依次接受00 00 00 00 00 00 00 FF,并存储起来.小端机器理解这个数据真实值就是"FF00 0000 0000 0000". 扩大了很多倍!!!!
	//因此需要字节序的转换.
	//本机字节序 => 网络字节序 => 网络字节序 => 本机字节序
	btyeInt = make([]byte, 8)
	binary.LittleEndian.PutUint64(btyeInt, 16)//[16 0 0 0 0 0 0 0]
	fmt.Println(btyeInt)
}
